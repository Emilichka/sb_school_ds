{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYZRf1ncz-sT"
      },
      "source": [
        "NLP2_1\n",
        "https://www.hackerrank.com/challenges/detect-the-email-addresses/problem?isFullScreen=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_2 = \"\"\"\n",
        "Email: bd@tnmbonline.com\n",
        "Email: ttn_tmbankhi@sancharnet.in\n",
        "Global Network of Branch / ATMпїЅпїЅпїЅs / Points of Presence:\n",
        "Click the Branch Network to find out more about the current branches / atms / pop network anwhere in India.\n",
        "Email: ibd@tnmbonline.com\n",
        "Other Non Banking Administrative Offices:\n",
        "Click the Non Banking Administrative Offices Network to find out more about the other offices / departments of TMB.\n",
        " http://wef.ch/m1ZAW  #WEF @NextBillion\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "pattern = r'([A-Za-z0-9\\._]+@[a-zA-Z0-9\\._-]+\\.[a-zA-Z0-9_-]+)'\n",
        "\n",
        "seq = re.compile(pattern)\n",
        "\n",
        "emails = set(seq.findall(test_2))\n",
        "print(';'.join(sorted(emails)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "pattern = r'([A-Za-z0-9\\._]+@[a-zA-Z0-9\\._-]+\\.[a-zA-Z0-9_-]+)'\n",
        "\n",
        "seq = re.compile(pattern)\n",
        "\n",
        "k = int(input())\n",
        "\n",
        "text = \"\\n\".join([input() for i in range(k)])\n",
        "emails = set(seq.findall(text))\n",
        "print(';'.join(sorted(emails)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKzbIfdq0CKr"
      },
      "source": [
        "NLP2_2 https://www.hackerrank.com/challenges/detect-the-domain-name/problem?isFullScreen=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_inp = \"\"\"id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b>^ [\"Train (noun)\"](http://www.askoxford.com/concise_oed/train?view=uk). <i>(definition – Compact OED)</i>. Oxford University Press<span class=\"reference-accessdate\">. Retrieved 2008-03-18</span>.</span><span title=\"ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&rft.atitle=Train+%28noun%29&rft.genre=article&rft_id=http%3A%2F%2Fwww.askoxford.com%2Fconcise_oed%2Ftrain%3Fview%3Duk&rft.jtitle=%28definition+%E2%80%93+Compact+OED%29&rft.pub=Oxford+University+Press&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal\" class=\"Z3988\"><span style=\"display:none;\"> </span></span></span></li>\n",
        "<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b>^</b></span> <span class=\"reference-text\"><span class=\"citation book\">Atchison, Topeka and Santa Fe Railway (1948). <i>Rules: Operating Department</i>. p. 7.</span><span title=\"ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&rft.au=Atchison%2C+Topeka+and+Santa+Fe+Railway&rft.aulast=Atchison%2C+Topeka+and+Santa+Fe+Railway&rft.btitle=Rules%3A+Operating+Department&rft.date=1948&rft.genre=book&rft.pages=7&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\" class=\"Z3988\"><span style=\"display:none;\"> </span></span></span></li>\n",
        "<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b>^ [Hydrogen trains](http://www.hydrogencarsnow.com/blog2/index.php/hydrogen-vehicles/i-hear-the-hydrogen-train-a-comin-its-rolling-round-the-bend/)</span></li>\n",
        "<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b>^ [Vehicle Projects Inc. Fuel cell locomotive](http://www.bnsf.com/media/news/articles/2008/01/2008-01-09a.html)</span></li>\n",
        "<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b>^</b></span> <span class=\"reference-text\"><span class=\"citation book\">Central Japan Railway (2006). <i>Central Japan Railway Data Book 2006</i>. p. 16.</span><span title=\"ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&rft.au=Central+Japan+Railway&rft.aulast=Central+Japan+Railway&rft.btitle=Central+Japan+Railway+Data+Book+2006&rft.date=2006&rft.genre=book&rft.pages=16&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook\" class=\"Z3988\"><span style=\"display:none;\"> </span></span></span></li>\n",
        "<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b>^ [\"Overview Of the existing Mumbai Suburban Railway\"](http://web.archive.org/web/20080620033027/http://www.mrvc.indianrail.gov.in/overview.htm). _Official webpage of Mumbai Railway Vikas Corporation_. Archived from [the original](http://www.mrvc.indianrail.gov.in/overview.htm) on 2008-06-20<span class=\"reference-accessdate\">. Retrieved 2008-12-11</span>.</span><span title=\"ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fen.wikipedia.org%3ATrain&rft.atitle=Overview+Of+the+existing+Mumbai+Suburban+Railway&rft.genre=article&rft_id=http%3A%2F%2Fwww.mrvc.indianrail.gov.in%2Foverview.htm&rft.jtitle=Official+webpage+of+Mumbai+Railway+Vikas+Corporation&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal\" class=\"Z3988\"><span style=\"display:none;\"> </span></span></span></li>\n",
        "</ol>\n",
        "</div>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "askoxford.com;bnsf.com;hydrogencarsnow.com;mrvc.indianrail.gov.in;web.archive.org\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# (http:\\/\\/)?(www\\.)?\\w+\\.(com|net|org)\n",
        "pattern = r\"https?://(?:ww(?:w|2)\\.)?([\\w\\.\\-]*\\.[a-zA-Z]+)\"\n",
        "\n",
        "seq = re.compile(pattern)\n",
        "sites = set(seq.findall(test_inp))\n",
        "print(';'.join(sorted(sites)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "pattern = r\"https?://(?:ww(?:w|2)\\.)?([\\w\\.\\-]*\\.[a-zA-Z]+)\"\n",
        "\n",
        "seq = re.compile(pattern)\n",
        "\n",
        "k = int(input())\n",
        "\n",
        "text = \"\\n\".join([input() for i in range(k)])\n",
        "sites = set(seq.findall(text))\n",
        "print(';'.join(sorted(sites)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op8qPHa8J68_"
      },
      "source": [
        "NLP2_3 (дз1): Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0BRC1-k81pIW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"labeled.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Собаке - собачья смерть\\n</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment  toxic\n",
              "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
              "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
              "2                          Собаке - собачья смерть\\n    1.0\n",
              "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
              "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "toxic\n",
              "0.0    9586\n",
              "1.0    4826\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.toxic.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'купить'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Для stemming and lemmatization так как тексты на русском языке воспользуемся библиотекой pymorpy\n",
        "\n",
        "import pymorphy3\n",
        "import re\n",
        "\n",
        "stemming  = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "stemming.parse(\"купила\")[0].normal_form\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stem_text(text):\n",
        "    words = re.split(r\"\\W+\", text)\n",
        "    stemming_words  = [stemming.parse(word)[0].normal_form for word in words]\n",
        "    return \" \".join(stemming_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stem_token(text):\n",
        "    words = re.split(r\"\\W+\", text)\n",
        "    stemming_words  = [stemming.parse(word)[0].normal_form for word in words]\n",
        "    return stemming_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"stem_comment\"] = df[\"comment\"].apply(stem_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"stem_token\"] = df[\"comment\"].apply(stem_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/emiliasr/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/emiliasr/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['NLTK', 'упрощает', 'обработку', 'текста', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"NLTK упрощает обработку текста.\"\n",
        "word_tokens = word_tokenize(text, language=\"russian\")\n",
        "print(word_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['лемматизирова', 'форм', 'слов', 'лист', 'эт', 'лист']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "text = \"Лемматизированная форма слова листья это лист\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "lemmatized_words = [stemmer.stem(word) for word in tokens]\n",
        "print(lemmatized_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Lemmatization\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lem_text(text):\n",
        "       nltk_tokens = nltk.word_tokenize(text)\n",
        "       lem_token  = [wordnet_lemmatizer.lemmatize(token) for token in nltk_tokens]\n",
        "       return \" \".join(lem_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"lem_token\"] = df[\"comment\"].apply(lem_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>toxic</th>\n",
              "      <th>lem_comment</th>\n",
              "      <th>stem_comment</th>\n",
              "      <th>lem_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Верблюдов-то за что ? Дебилы , бл ...</td>\n",
              "      <td>верблюд то за что дебил бл</td>\n",
              "      <td>Верблюдов-то за что ? Дебилы , бл ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Хохлы , это отдушина затюканого россиянина , м...</td>\n",
              "      <td>хохол это отдушина затюканый россиянин мол вон...</td>\n",
              "      <td>Хохлы , это отдушина затюканого россиянина , м...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Собаке - собачья смерть\\n</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Собаке - собачья смерть</td>\n",
              "      <td>собака собачий смерть</td>\n",
              "      <td>Собаке - собачья смерть</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Страницу обнови , дебил . Это тоже не оскорбле...</td>\n",
              "      <td>страница обновить дебил это тоже не оскорблени...</td>\n",
              "      <td>Страницу обнови , дебил . Это тоже не оскорбле...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>тебя не убедил 6-страничный пдф в том , что Ск...</td>\n",
              "      <td>ты не убедить 6 страничный пдф в тот что скрип...</td>\n",
              "      <td>тебя не убедил 6-страничный пдф в том , что Ск...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment  toxic  \\\n",
              "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0   \n",
              "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0   \n",
              "2                          Собаке - собачья смерть\\n    1.0   \n",
              "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0   \n",
              "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0   \n",
              "\n",
              "                                         lem_comment  \\\n",
              "0              Верблюдов-то за что ? Дебилы , бл ...   \n",
              "1  Хохлы , это отдушина затюканого россиянина , м...   \n",
              "2                            Собаке - собачья смерть   \n",
              "3  Страницу обнови , дебил . Это тоже не оскорбле...   \n",
              "4  тебя не убедил 6-страничный пдф в том , что Ск...   \n",
              "\n",
              "                                        stem_comment  \\\n",
              "0                        верблюд то за что дебил бл    \n",
              "1  хохол это отдушина затюканый россиянин мол вон...   \n",
              "2                             собака собачий смерть    \n",
              "3  страница обновить дебил это тоже не оскорблени...   \n",
              "4  ты не убедить 6 страничный пдф в тот что скрип...   \n",
              "\n",
              "                                           lem_token  \n",
              "0              Верблюдов-то за что ? Дебилы , бл ...  \n",
              "1  Хохлы , это отдушина затюканого россиянина , м...  \n",
              "2                            Собаке - собачья смерть  \n",
              "3  Страницу обнови , дебил . Это тоже не оскорбле...  \n",
              "4  тебя не убедил 6-страничный пдф в том , что Ск...  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['I', 'love', 'this', 'product'],\n",
              " ['This', 'is', 'a', 'bad', 'product'],\n",
              " ['I', 'dislike', 'this'],\n",
              " ['This', 'is', 'the', 'best', '!']]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts = [\"I love this product\", \"This is a bad product\", \"I dislike this\", \"This is the best!\"]\n",
        "labels = [1, 0, 0, 1]  # 1 - позитивный, 0 - негативный\n",
        "\n",
        "# Токенизация\n",
        "tokens = [word_tokenize(text) for text in texts]\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Токенизация\n",
        "tokens = [word_tokenize(text) for text in texts]\n",
        "\n",
        "# Создание BoW модели\n",
        "vectorizer = CountVectorizer()\n",
        "bow = vectorizer.fit_transform([' '.join(token) for token in tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "#BOW variant 2\n",
        "import nltk\n",
        "\n",
        "# create the vocabulary\n",
        "vocab = set()\n",
        "\n",
        "# create the bag-of-words model\n",
        "bow_model = []\n",
        "\n",
        "for text in df.stem_comment.values:\n",
        "    # create a dictionary to store the word counts\n",
        "    word_counts = {}\n",
        "\n",
        "    # tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # update the vocabulary\n",
        "    vocab.update(tokens)\n",
        "\n",
        "    # count the occurrences of each word\n",
        "    for word in tokens:\n",
        "        if word in word_counts:\n",
        "            word_counts[word] += 1\n",
        "        else:\n",
        "            word_counts[word] = 1\n",
        "\n",
        "    # add the word counts to the bag-of-words model\n",
        "    bow_model.append(word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#BOW variant 1\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# create the vocabulary\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "\n",
        "# create the bag-of-words model\n",
        "bow_model_sk = vectorizer.fit_transform(df.stem_comment.values)\n",
        "\n",
        "# print the bag-of-words model\n",
        "print(bow_model_sk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NLP2_4 Реализовать классификатор токсичных комментариев tfidf на базе датасета\n",
        "https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments  \n",
        "\n",
        "Дубликат файла: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>toxic</th>\n",
              "      <th>lem_comment</th>\n",
              "      <th>stem_comment</th>\n",
              "      <th>lem_token</th>\n",
              "      <th>stem_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Верблюдов-то за что ? Дебилы , бл ...</td>\n",
              "      <td>верблюд то за что дебил бл</td>\n",
              "      <td>Верблюдов-то за что ? Дебилы , бл ...</td>\n",
              "      <td>[верблюд, то, за, что, дебил, бл, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Хохлы , это отдушина затюканого россиянина , м...</td>\n",
              "      <td>хохол это отдушина затюканый россиянин мол вон...</td>\n",
              "      <td>Хохлы , это отдушина затюканого россиянина , м...</td>\n",
              "      <td>[хохол, это, отдушина, затюканый, россиянин, м...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Собаке - собачья смерть\\n</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Собаке - собачья смерть</td>\n",
              "      <td>собака собачий смерть</td>\n",
              "      <td>Собаке - собачья смерть</td>\n",
              "      <td>[собака, собачий, смерть, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Страницу обнови , дебил . Это тоже не оскорбле...</td>\n",
              "      <td>страница обновить дебил это тоже не оскорблени...</td>\n",
              "      <td>Страницу обнови , дебил . Это тоже не оскорбле...</td>\n",
              "      <td>[страница, обновить, дебил, это, тоже, не, оск...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment  toxic  \\\n",
              "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0   \n",
              "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0   \n",
              "2                          Собаке - собачья смерть\\n    1.0   \n",
              "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0   \n",
              "\n",
              "                                         lem_comment  \\\n",
              "0              Верблюдов-то за что ? Дебилы , бл ...   \n",
              "1  Хохлы , это отдушина затюканого россиянина , м...   \n",
              "2                            Собаке - собачья смерть   \n",
              "3  Страницу обнови , дебил . Это тоже не оскорбле...   \n",
              "\n",
              "                                        stem_comment  \\\n",
              "0                        верблюд то за что дебил бл    \n",
              "1  хохол это отдушина затюканый россиянин мол вон...   \n",
              "2                             собака собачий смерть    \n",
              "3  страница обновить дебил это тоже не оскорблени...   \n",
              "\n",
              "                                           lem_token  \\\n",
              "0              Верблюдов-то за что ? Дебилы , бл ...   \n",
              "1  Хохлы , это отдушина затюканого россиянина , м...   \n",
              "2                            Собаке - собачья смерть   \n",
              "3  Страницу обнови , дебил . Это тоже не оскорбле...   \n",
              "\n",
              "                                          stem_token  \n",
              "0                [верблюд, то, за, что, дебил, бл, ]  \n",
              "1  [хохол, это, отдушина, затюканый, россиянин, м...  \n",
              "2                        [собака, собачий, смерть, ]  \n",
              "3  [страница, обновить, дебил, это, тоже, не, оск...  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"stem_comment\"], df.toxic, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# Define the TfidfVectorizer with specified parameters\n",
        "vec = TfidfVectorizer()\n",
        "\n",
        "\n",
        "# Transform the training, validation, and test data\n",
        "trn_term_doc = vec.fit_transform(X_train)\n",
        "test_term_doc = vec.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def ROC_curve_plot(datatest, prediction, classes, figure_title):\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    nbr_classes = len(classes)\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    y = np.zeros(nbr_classes*len(datatest))\n",
        "    y_hat = np.zeros(nbr_classes*len(datatest))\n",
        "\n",
        "    for idx,clas in enumerate(classes):\n",
        "        print('... Processing {}'.format(clas))\n",
        "        print('Cofusion Matrix:\\n', confusion_matrix(datatest[clas], prediction[:,idx]))\n",
        "        fpr[clas], tpr[clas], _ = roc_curve(datatest[clas], prediction[:,idx])\n",
        "        roc_auc[clas] = auc(fpr[clas], tpr[clas])\n",
        "\n",
        "        y[idx*len(datatest):(idx+1)*len(datatest)] = datatest[clas].values\n",
        "        y_hat[idx*len(datatest):(idx+1)*len(datatest)] = prediction[:,idx]\n",
        "\n",
        "    # Compute average ROC curve and ROC area\n",
        "    fpr[\"all\"], tpr[\"all\"], _ = roc_curve(y, y_hat)\n",
        "    roc_auc[\"all\"] = auc(fpr[\"all\"], tpr[\"all\"])\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in [\"all\"] + classes:\n",
        "        plt.plot(fpr[i], tpr[i], label='{0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate' , fontsize=12)\n",
        "    plt.title(figure_title,           fontsize=12)\n",
        "    plt.legend(loc=\"lower right\",     fontsize=12)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "\n",
        "XGBoost_pipeline = Pipeline([\n",
        "                            ('tfidf', TfidfVectorizer()),\n",
        "                            ('clf', OneVsRestClassifier(XGBClassifier(), n_jobs=1)),\n",
        "                           ])\n",
        "\n",
        "XGBoost_pipeline.fit(X_train,y_train)\n",
        "prediction = XGBoost_pipeline.predict(X_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4324,)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC AUC for class: 0.7631441501433096\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# Calculate ROC AUC score for current class and store\n",
        "roc_auc = roc_auc_score(y_test, prediction)\n",
        "# Print ROC AUC scores for each class\n",
        "print(f\"ROC AUC for class: {roc_auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
